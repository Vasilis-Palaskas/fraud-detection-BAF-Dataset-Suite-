{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f95e9af1",
   "metadata": {},
   "source": [
    "# Pipeline Creation & Check for similarity between validation and test datasets.\n",
    "In this notebook, the purpose is to cover the following ones:\n",
    "\n",
    "1) Select cross-validation scheme\n",
    "2) Check adversarial validation of validation and test sets\n",
    "3) Create pipeline including data processing, transformations, hyper-parameter search and evaluation on the validation test sets\n",
    "\n",
    "In the next notebook, we will finally evaluate the model to the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c24192",
   "metadata": {},
   "source": [
    "## Data importing & Pipeline creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "759352d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "# Manually restore the alias removed in NumPy 2.0\n",
    "np.NaN = np.nan\n",
    "np.bool = bool\n",
    "np.float = float\n",
    "import scipy.stats\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from pathlib import Path\n",
    "DATADIR= Path(\"/workspaces/fraud-detection-BAF-Dataset-Suite-/FraudDataset\")\n",
    "data_dir =DATADIR\n",
    "if not data_dir.exists():\n",
    "    raise FileNotFoundError(f\"Data directory not found: {data_dir}\")\n",
    "extension = \"csv\"  # Change to \"csv\" if needed\n",
    "data_paths = [str(p) for p in sorted(data_dir.glob(f\"*.{extension}\"))]\n",
    "data_paths[0]\n",
    "dataset=pd.read_csv(data_paths[0])\n",
    "train_dataset=dataset.loc[dataset[\"month\"]<6,:].copy()\n",
    "test_dataset=dataset.loc[dataset[\"month\"]>=6,:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9474d98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe2011f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fraud_bool', 'income', 'name_email_similarity',\n",
       "       'prev_address_months_count', 'current_address_months_count',\n",
       "       'customer_age', 'days_since_request', 'intended_balcon_amount',\n",
       "       'payment_type', 'zip_count_4w', 'velocity_6h', 'velocity_24h',\n",
       "       'velocity_4w', 'bank_branch_count_8w',\n",
       "       'date_of_birth_distinct_emails_4w', 'employment_status',\n",
       "       'credit_risk_score', 'email_is_free', 'housing_status',\n",
       "       'phone_home_valid', 'phone_mobile_valid', 'bank_months_count',\n",
       "       'has_other_cards', 'proposed_credit_limit', 'foreign_request', 'source',\n",
       "       'session_length_in_minutes', 'device_os', 'keep_alive_session',\n",
       "       'device_distinct_emails_8w', 'device_fraud_count', 'month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e64d4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "fraud_bool",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "bank_branch_count_8w",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "60f40149-7f3e-4bfc-aec8-f3f7b9dc4e33",
       "rows": [
        [
         "0",
         "5"
        ],
        [
         "0",
         "3"
        ],
        [
         "0",
         "15"
        ],
        [
         "0",
         "11"
        ],
        [
         "0",
         "1"
        ],
        [
         "0",
         "705"
        ],
        [
         "0",
         "28"
        ],
        [
         "0",
         "6"
        ],
        [
         "0",
         "2"
        ],
        [
         "0",
         "14"
        ],
        [
         "0",
         "11"
        ],
        [
         "0",
         "2"
        ],
        [
         "0",
         "692"
        ],
        [
         "0",
         "18"
        ],
        [
         "0",
         "28"
        ],
        [
         "0",
         "12"
        ],
        [
         "0",
         "26"
        ],
        [
         "0",
         "1"
        ],
        [
         "0",
         "1839"
        ],
        [
         "0",
         "1590"
        ],
        [
         "0",
         "1"
        ],
        [
         "0",
         "378"
        ],
        [
         "0",
         "22"
        ],
        [
         "0",
         "11"
        ],
        [
         "0",
         "585"
        ],
        [
         "0",
         "0"
        ],
        [
         "0",
         "0"
        ],
        [
         "0",
         "27"
        ],
        [
         "0",
         "16"
        ],
        [
         "0",
         "10"
        ],
        [
         "0",
         "6"
        ],
        [
         "0",
         "9"
        ],
        [
         "0",
         "2"
        ],
        [
         "0",
         "2"
        ],
        [
         "0",
         "233"
        ],
        [
         "0",
         "26"
        ],
        [
         "0",
         "1"
        ],
        [
         "0",
         "0"
        ],
        [
         "0",
         "5"
        ],
        [
         "0",
         "5"
        ],
        [
         "0",
         "25"
        ],
        [
         "0",
         "9"
        ],
        [
         "0",
         "2"
        ],
        [
         "1",
         "2"
        ],
        [
         "0",
         "1"
        ],
        [
         "0",
         "1"
        ],
        [
         "0",
         "6"
        ],
        [
         "0",
         "19"
        ],
        [
         "0",
         "189"
        ],
        [
         "0",
         "1"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 794989
       }
      },
      "text/plain": [
       "fraud_bool\n",
       "0     5\n",
       "0     3\n",
       "0    15\n",
       "0    11\n",
       "0     1\n",
       "     ..\n",
       "0     2\n",
       "0    13\n",
       "0    14\n",
       "0     0\n",
       "0    10\n",
       "Name: bank_branch_count_8w, Length: 794989, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"bank_branch_count_8w\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d97261",
   "metadata": {},
   "source": [
    "Create necessary classes for pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dac893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImportantColumnsDropper:\n",
    "    def __init__(self, columns_to_drop):\n",
    "        self.columns_to_drop = columns_to_drop\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=self.columns_to_drop, errors='ignore')\n",
    "\n",
    "class ContinuousTransformer:\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X[self.columns])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed[self.columns] = self.scaler.transform(X[self.columns])\n",
    "        return X_transformed\n",
    "    \n",
    "class CategoricalTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom transformer for all categorical/ordinal feature engineering\n",
    "    as described in the exploratory analysis section.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Store mappings if needed for transform\n",
    "        self.income_threshold = 0.8\n",
    "        self.age_threshold = 40\n",
    "        self.payment_types = ['AA', 'AC']\n",
    "        self.employment_types = ['CA', 'CB', 'CC']\n",
    "        self.intended_balcon_merge = ['52-92', '92+']\n",
    "        self.bank_branch_count_8w_binned_merge = ['<=42']\n",
    "        # No fitting required for these rules, but for pipeline compatibility\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # No fitting needed, but method required for pipeline\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        # 1. income_level\n",
    "        if 'income' in X.columns:\n",
    "            X['income_level'] = np.where(X['income'] < self.income_threshold, '<0.8', '0.8')\n",
    "            X.drop(columns='income', inplace=True)\n",
    "\n",
    "        # 2. customer_age_binned\n",
    "        if 'customer_age' in X.columns:\n",
    "            X['customer_age_binned'] = np.where(X['customer_age'] > self.age_threshold, '>40 (More fraud)', '<=40 (less fraud)')\n",
    "            X.drop(columns='customer_age', inplace=True)\n",
    "\n",
    "        # 3. payment_type_engineered\n",
    "        if 'payment_type' in X.columns:\n",
    "            X['payment_type_engineered'] = np.where(~X['payment_type'].isin(self.payment_types), 'Other', X['payment_type'])\n",
    "            X.drop(columns='payment_type', inplace=True)\n",
    "\n",
    "        # 4. employment_status_engineered\n",
    "        if 'employment_status' in X.columns:\n",
    "            X['employment_status_engineered'] = np.where(~X['employment_status'].isin(self.employment_types), 'CD_CE_CF_CG', X['employment_status'])\n",
    "            X.drop(columns='employment_status', inplace=True)\n",
    "\n",
    "        # 5. intended_balcon_amount_binned_engineered\n",
    "        if 'intended_balcon_amount_binned' in X.columns:\n",
    "            X['intended_balcon_amount_binned_engineered'] = np.where(\n",
    "                X['intended_balcon_amount_binned'].isin(self.intended_balcon_merge),\n",
    "                '52+',\n",
    "                X['intended_balcon_amount_binned']\n",
    "            )\n",
    "            X.drop(columns='intended_balcon_amount_binned', inplace=True)\n",
    "\n",
    "        # 7. Drop columns as per analysis (if present)\n",
    "        drop_cols = [\"income\", \"customer_age\", \"payment_type\", \"employment_status\",\"intended_balcon_amount\"]\n",
    "        for col in drop_cols:\n",
    "            if col in X.columns:\n",
    "                X.drop(columns=col, inplace=True)\n",
    "\n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850526c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('drop_unimportant_columns', ImportantColumnsDropper(columns_to_drop=['proposed_credit_limit', 'valid_transferred_amount', 'foreign_request', \n",
    "                                                                        'days_since_request','bank_branch_count_8w','velocity_6h',\n",
    "                                                                        'velocity_24h', 'velocity_4w','zip_count_4w',\n",
    "                                                                        'session_length_in_minutes','source','current_address_months_count',\n",
    "                                                                        'device_fraud_count'\n",
    "                                                                        ]),\n",
    "    ('transformation_categoricals', CategoricalTransformer(columns=['merchant_id', 'customer_id'])),\n",
    "    ('imputation', SimpleImputer(strategy='median')),\n",
    "    ('logarithmic_transformation', LogTransformer(columns=['amount'])),"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
